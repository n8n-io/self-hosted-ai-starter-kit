# ========================
# PostgreSQL Settings
# ========================
POSTGRES_VERSION=16-alpine                  # Specify PostgreSQL version here
POSTGRES_PORT=5432
POSTGRES_USER=n8n_user                        # Username for PostgreSQL
POSTGRES_PASSWORD=n8n_password                 # Password for PostgreSQL
POSTGRES_DB=n8n_db                             # Database name for n8n
POSTGRES_HOSTNAME=postgres

# ========================
# n8n Settings
# ========================

############
# [required] 
# n8n credentials - use the command `openssl rand -hex 32` to generate both
#   openssl is available by default on Linux/Mac
#   For Windows, you can use the 'Git Bash' terminal installed with git
#   Or run the command: python -c "import secrets; print(secrets.token_hex(32))"
############

N8N_VERSION=latest            # n8n version (can be customized)
N8N_PORT=5678
N8N_HOSTNAME=n8n
N8N_ENCRYPTION_KEY=super-secret-key         # Encryption key for securing sensitive data in n8n
N8N_USER_MANAGEMENT_JWT_SECRET=even-more-secret  # JWT secret for user management in n8n
N8N_DEFAULT_BINARY_DATA_MODE=filesystem     # Binary data mode (options: 'filesystem', 'database')
# n8n Diagnostics & Personalization Settings
N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
N8N_DIAGNOSTICS_ENABLED=true              # Select false to Disable diagnostics in n8n for privacy
N8N_PERSONALIZATION_ENABLED=true          # Select false to Disable personalization in n8n for privacy
N8N_RUNNERS_ENABLED=true     # Enable n8n runners for parallel execution

# ========================
# Ollama Settings
# ========================
OLLAMA_VERSION=latest     # Ollama version (can be customized)
OLLAMA_PORT=11434
# Uncomment and set the value if using Ollama locally (e.g., on macOS or custom setup)
# OLLAMA_HOST=host.docker.internal:11434    # Host address for Ollama (macOS users or local setup)
OLLAMA_HOST=ollama:11434         # Use a custom host for Ollama connection
# Ollama Service Selection (choose one of the available services)
OLLAMA_SERVICE=ollama-gpu  # Select from ollama-cpu, ollama-gpu, ollama-gpu-amd 
OLLAMA_CONTEXT_LENGTH=8192
OLLAMA_FLASH_ATTENTION=1
OLLAMA_KV_CACHE_TYPE=q8_0
OLLAMA_MAX_LOADED_MODELS=2

# ========================
# Qdrant Settings (Optional)
# ========================
QDRANT_VERSION=latest        # Qdrant version (can be customized)
QDRANT_PORT_1=6333 # for REST API and Web UI
QDRANT_PORT_2=6334 # for GRPC API
QDRANT_HOSTNAME=qdrant

# ========================
# Open Web UI Settings (Optional)
# ========================
OPEN_WEBUI_VERSION=main # Open Web UI Docker image (default: main)
OPEN_WEBUI_PORT=8088
OPEN_WEBUI_HOSTNAME=open-webui
#OPEN_WEBUI_SECRET_KEY=your_open_webui_api_key  # Add your Open Web UI API key here

# ========================
# Redis Settings (Optional)
# ========================
REDIS_VERSION=latest        # Redis version (can be customized)
REDIS_PORT=6379
REDIS_HOSTNAME=redis

# ========================
# SearXNG Settings needs Redis
# ========================
SEARXNG_VERSION=latest        # Redis version (can be customized)
SEARXNG_PORT=8181

# By default listen on https://localhost
# To change this:
# uncomment SEARXNG_HOSTNAME, and replace <host> by the SearXNG hostname
# uncomment LETSENCRYPT_EMAIL, and replace <email> by your email (require to create a Let's Encrypt certificate)

SEARXNG_HOSTNAME=searxng
# LETSENCRYPT_EMAIL=<email>

# Optional:
# If you run a very small or a very large instance, you might want to change the amount of used uwsgi workers and threads per worker
# More workers (= processes) means that more search requests can be handled at the same time, but it also causes more resource usage
SEARXNG_UWSGI_WORKERS=4
SEARXNG_UWSGI_THREAD=4
